%        File: exponential.tex
%     Created: Tue Feb 16 08:00 PM 2021 P
% Last Change: Tue Feb 16 08:00 PM 2021 P
%
\documentclass[a4paper]{article}
\usepackage[]{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\newtheorem{problem}{Problem}[section]

\begin{document}

\section{Notation}


$X$ random variable \\ 
$x$ instantiation/sample of the random variable $X$ \\
$u(X_1, \ldots, X_N)$ sufficient statistic \\
$\theta$ parameterization of the distribution \\
$\eta$ natural parameter \\
$Z(\eta)$ normalization factor \\
$g(\eta)$ inverse normalizer \\
$h(x)$ intrinsic/carrier measure

\section{Pitman-Koopman-Dermois Theorem}

Suppose a sufficient statistic is additive: $ u(X_1,\ldots,X_N) = \sum_{n=1}^N u(X_n) $.  Furthermore assume it is a minimal sufficient statistic, which means mathematically there exists a mapping from any other sufficient statistic to $u$.

\begin{equation}
  f( u'(X_1,\ldots,X_N) ) = u(X_1,\ldots,X_N) )
  \label{}
\end{equation}

Then we can express parametrize the distribution by the natural parameters, $\eta$, such that

\begin{equation}
  P( x \vert \eta) = g(\eta) h(x) e^{ \left< u(x), \eta \right>}
  \label{}
\end{equation} 

where 

\begin{equation}
  \begin{split}
    \tilde{P}(X) &:= h(x) e^{ \left< u(x), \eta \right> } \\
    Z(\eta) &:= \int h(x) e^{ \left< u(x), \eta \right> } dX \\
    g(\eta) &:= \frac{1}{Z(\eta)}
  \end{split}
  \label{}
\end{equation}

\begin{problem} 
Find a probability distribution that has a sufficient statistic, but is not an exponential family distribution.  Is it an additive sufficient statistic?
\end{problem}

\begin{problem}
  What is the difference between a normal distribution and an exponential family of distributions?
\end{problem}

The following are two more complicated distributions.  Challenge yourself to see if you can convert them to exponential family form.

\begin{problem}
  The Weibull distribution was used to model the distribution for the onset of covid cases.  \\
  The Weibull distribution has the cumulative distribution function
  \begin{equation}
    P( X \leq x \vert \lambda, k ) = \begin{cases}
      1-e^{-\frac{x}{\lambda}^k} &, x \geq 0 \\
      0 &, x <0
    \end{cases}
    \label{}
  \end{equation}

Express the Weibull distribution as an exponential family, with a sufficient statistic and natural parameters.
\end{problem}

\begin{problem}
  The Gumbel distribution, which is used for the Gumbel-softmax trick for backpropagating through categorical distributions, is defined as
  \begin{equation}
    \begin{split}
      P( x \vert \mu, \beta ) = \frac{1}{\beta} e^{-(z+e^{-z})}, \quad z:= \frac{x-\mu}{\beta} \\
      P( X \leq x \vert \mu, \beta ) = e^{-e^{-\frac{x-\mu}{\beta}} }
    \end{split}
    \label{Gumbel}
  \end{equation}
Is it an exponential family?  If so, find a minimal sufficient statistic and natural parameters.
\end{problem}

The following are more conceptual questions.

\begin{problem}
  Bishop shows that the family of Bernoulli distributions is an exponential family (and Bernoulli is a special case of the binomial distribution).  However, there is a mistake/problem with Bishop's definition, what is it??
\end{problem}

\begin{problem}
  Give an exponential family that contains the binomial distribution (discrete distribution of n coin flips with the probability of heads being p).
\end{problem}

\begin{problem}
  Given a pdf, we $p(x \vert \theta)$, we can write $p(x) = e^{\log p( x \vert \theta)}$.  This seems to suggest any pdf can be turned into an exponential family.  But that is not quite correct.  Give conditions when this trick will and will not work.
\end{problem}

\begin{problem}
Show that an exponential family distribution satisfies the Fisher-Neyman factorization theorem

\begin{equation}
  p(x \vert \theta) = h(x) f_\theta( u(x) )
  \label{}
\end{equation}

where $h(x)$,$f_\theta$ are non-negative functions.

\end{problem}


\section{Sufficiency}

This is a more advanced, theoretical section (as in you may want to skip this for later), which is meant to solidify our understanding of the mysterious concept of sufficiency.  An understanding of this is important for ideas about perfect and imperfect data reduction, such as Tishby's Information Bottleneck. \\

One explanation of a sufficient statistic is that it contains all relevant information about observations for inferring the parametrization.  We wish to understand what this means.

\begin{problem}
Suppose we know nothing about our data distribution, $P_{data}(X)$.  What would be an appropriate sufficient statistic?  \\
What do we need to specify beforehand before we can define a sufficient statistic?  
\end{problem}

\begin{problem}
  Suppose we have two sets of samples of size $N$, $\{x_1,\ldots,x_N\},\{x_1',\ldots,x_N'\}$ from a normal distribution $\mathcal{N}( \mu, \sigma^2 )$, where $\sigma^2$ is known but $\mu$ is unknown.  \\
  Further suppose that we have a normal prior, $P(\mu) = \mathcal{N}(0,1)$.  \\
  The sufficient statistic is the sum of the samples, so 
\begin{equation}
  \begin{split}
    u(x_1,\ldots,x_N) = \sum_n x_n \\
    = u(x_1',\ldots,x_N') = \sum_n x_n' \\
  \end{split}
  \label{}
\end{equation}
Is it true that we assume equal posterior probabilities?
\begin{equation}
  P( \mu = 1 \vert x_1,\ldots,x_N ) = P( \mu = 2 \vert x_1', \ldots, x_N')
  \label{}
\end{equation}
Suppose the sufficient statistic is the same.  How do the posteriors compare?
\begin{equation}
  P( \mu = 1 \vert x_1,\ldots,x_N, u(x_1,\ldots,x_N) = C ) = P( \mu = 2 \vert x_1',\ldots,x_N', u(x_1',\ldots,x_N') = C ) ?
  \label{}
\end{equation}

\end{problem}

\begin{problem}
Suppose two sets of samples of size $N$, $\{x_1,\ldots,x_N\},\{x_1',\ldots,x_N'\}$, from a family of distributions parametrized by $\theta$ with sufficient statistic $u$.

Suppose $u(x_1,\ldots,x_N) = u(x_1',\ldots,x_N')$.  Can you find an example where $ \prod_n P(x_n \vert \theta) \neq \prod_n P(x_n \vert \theta') $?
\end{problem}

\begin{problem}
  Suppose now that we now have two gaussian distributions with the same variance, $\mathcal{N}(-1,1)$ and $\mathcal{N}(1,1)$.  We pick with equal probability one of these distributions but without knowing which one, and then we can sample from it repeatedly. \\
Now we repeatedly to sample this distribution $N$ times, but we reject/discard and resample any samples with $x<0$ (but we always end up with a total of $N$ accepted samples).

The sufficient statistic is still the mean (you can try to justify this, though it might be hard).

Can you come up with an example of $\{x_1,\ldots,x_N\}, \{x_1',\ldots,x_N'\}$ with the same sufficient statistic but different likelihoods?
\end{problem}

We can conclude

\begin{equation}
  P( x_1, \ldots, x_N \vert \theta, u(x_1,\ldots,x_N) = C ) \neq P( x_1', \ldots, x_N' \vert \theta, u(x_1',\ldots,x_N') = C )
  \label{}
\end{equation}

So sufficiency does not mean that given all samples with the same sufficient statistic are equally likely.

It does not mean that a sample has the same likelihood regardless of the parametrization (see ancillary statistic).

It does mean that the likelihood ratio of two samples with the same sufficient statistic is the same no matter what the parametrization:

\begin{equation}
  \frac{\prod_n P(x_n \vert \theta) }{ \prod_n P( x_n' \vert \theta) } = \frac{\prod_n P( x_n \vert \theta') }{ \prod_n P( x_n' \vert \theta') }
  \label{}
\end{equation}

That is, $P( x_1, \ldots, x_N \vert u(x_1, \ldots, x_N) \perp \theta $.

It also means that 

\begin{equation}
  \frac{\prod_n P(x_n \vert \theta) }{ \prod_n P( x_n \vert \theta') } = \frac{\prod_n P( x_n' \vert \theta) }{ \prod_n P( x_n' \vert \theta') }
  \label{}
\end{equation}

\begin{problem}
Explain what this equation means in terms of inferring the parameters given the sufficient statistic.
\end{problem}

\begin{problem}
Assuming the above, prove the Fisher-Neyman factorization theorem.

\begin{equation}
  p(x \vert \theta) = h(x) f_\theta( u(x) )
  \label{Fisher-Neyman factorization theorem}
\end{equation}

where $h(x)$,$f_\theta$ are non-negative functions.
\end{problem}

\begin{problem}
Draw a (graphical) relationship between a sample, $\mathcal{D} = X_1,\ldots,X_N)$, the sufficient statistic $u(x_1, \ldots, x_N)$, and the parametrization, $\theta$. 
Use a double forward arrow to indicate deterministic relationships.
\end{problem}


\section{Gradients of exponential families and statistical modeling}

\subsection{Gradients}

The first three problems are very fundamental to exponential families and machine learning, and have some level of progression.

\begin{problem}
Derive an expression for $ \nabla_\eta \log Z(\eta) $ (Hint: in terms of the sufficient statistics). \\
Derive an expression for $ \nabla_\eta \log Z(\eta) $ from the fact that $ \int P( x \vert \eta) dx = 1 $.
\end{problem}

\begin{problem}
Derive an expression for $ \nabla_\eta \nabla_\eta \log Z(\eta) $ in terms of the sufficient statistics.
\end{problem}

\begin{problem}
Derive an expression for $ \nabla_\eta \log P( x \vert \eta ) $ in terms of the expected sufficient statistic. \\
Suppose we have samples $\left\{ X_1, \ldots, X_N \right\}$ from a data distribution $P_{data}(X)$. We can define an empirical distribution, $ P_{emp}(X) = \frac{1}{N} \sum_{n=1}^N \delta(X = X_n) $.  \\
Derive an expression for

\begin{equation}
  \nabla_\eta KL\left( P_{emp}(X) \Vert P( X \vert \eta) \right) = \nabla_\eta \int P_{emp}(X) \log \frac{P_{emp}(X)}{P( X \vert \eta) } dX
  \label{}
\end{equation}
\end{problem}

\subsection{Discriminative Modeling}
The next three problems are increasingly more complex versions of the same problem.  Choose one.

\begin{problem}
  Suppose we have online samples of a Bernoulli distribution with an unknown probability.  Derive a first-order and Newton's method second-order gradient-based update of the distribution parameters.
\end{problem}

\begin{problem}
  Suppose we have a online samples ${(X,Y)}$ for a logistic regression model
  \begin{equation}
    P( Y \vert X, W) = \sigma( \sum_k w_k X_k )
    \label{logistic}
  \end{equation}
  Derive a first order and second order gradient-based update for the parameters $W$.
\end{problem}

\begin{problem} Generalized Linear Models (GLM) \\
  Derive a gradient update for $P_W( Y \vert X) \in Poisson(\lambda) $, where $\eta = \sum_k w_k X_k )$.
\end{problem}

\subsection{Convex Analysis}

The next two problems are more theoretical and are optional if you're not overly concerned about convex analysis.
\begin{problem}
Prove that $ \log Z(\eta)$ is a convex function. 
(Hint: use results from the above problem)  \\
Prove that the domain of natural parameters is convex.  \\
Assuming that the covariance of the sufficient statistic is positive definite ( equivalently, that the variance constrained along any vector is positive), prove that there is a one-to-one relationship between $\eta$ and the expected sufficient statistic.
\end{problem}

\begin{problem}
Assuming the above, prove the canonical link function, $f( E_{ x \sim P( x \vert \eta) } [ u(x) ] ) = \eta $, exists and is one-to-one.
\end{problem}

\subsection{KL divergence and M-projections}

\begin{equation}
  KL \left( P(X) \Vert Q(X) \right) := \int P(X) \log \frac{P(X)}{Q(X)} dX
  \label{KL}
\end{equation}

\begin{problem}
Derive a simplified expression for $ KL\left( P( x \vert \eta') \Vert P( x \vert \eta) \right)$ in terms of the natural parameters and expected sufficient statistics.  \\
What is $ KL\left( \mathcal{N} \left( \mu_1,\Sigma_1 \right) \Vert \mathcal{N} \left( \mu_2, \Sigma_2 \right) \right) $ ?
\end{problem}

\begin{problem}
Moment matching / M-projections \\
What is 
\begin{equation}
  \nabla_\eta KL\left( P( X \vert \eta') \Vert P( X \vert \eta) \right)
  \label{}
\end{equation}
(Hint: do not use the final result of the above problem).
Using gradients / derivatives or the previous problem, derive a condition for the M-projection:

\begin{equation}
  \text{argmin}_\eta KL \left( P_{data}(X) \Vert P( X \vert \eta) \right) = \text{argmin}_\eta \int P_{data}(X) \log \frac{ P_{data}(X) }{ P( X \vert \eta) } dX
  \label{}
\end{equation}
\end{problem}

\begin{problem}
  Derive the M-projection of a probability distribution $P(X)$ on to the space of Gaussian distributions.  \\
  What if you only had samples of $P(X)$?  How would you estimate the M-projection?
\end{problem}

\section{Probabilistic Graphical Models}

Undirected Markov networks are defined by $\mathcal{M} = \left( \mathcal{H}, \Phi \right) $ are defined by their undirected graph $\mathcal{H}$ and their factors, $\Phi = \{ \phi_i(D_i) \}_i$.  
The unnormalized probability distribution is called the Gibbs distribution:

\begin{equation}
  \tilde{P}_\Phi(\mathcal{X}) = \prod_i \phi_i(D_i)
  \label{Gibbs distribution}
\end{equation}

\begin{problem}
  If each of the factors is an exponential family distribution, write the joint distribution as an exponential family.  \\
\end{problem}

\section{Exponential families and convex optimization duality}

The convex conjugate / Fenchel transform / Legendre-Fenchel transform is the function to a (potentially non-convex) function.

A notational note, we will use lowercase $x$ here for random variables, instead of instantiations, since we are not considering samples in this section.

\begin{equation}
  f^*( \lambda ) = \sup_{x \in X} \left( \left< \lambda, x \right> - f(x) \right)
  \label{convex conjugate}
\end{equation}

Let $G(\eta)$ be the log normalizer:

\begin{equation}
  G(\eta) = \log Z(\eta) = \int h(x) e^{ \left< u(x), \eta \right> } dx
  \label{}
\end{equation}

Then we can re-expresss the exponential family formula as

\begin{equation}
  P( x \vert \eta) = h(x) e^{ \left< u(x), \eta \right> - G(\eta) }
  \label{}
\end{equation}

Let's consider the convex conjugate of the log normalizer:

\begin{equation}
  F( \lambda ) = \sup_\eta \left( \left< \lambda, \eta \right> - G(\eta) \right)
  \label{}
\end{equation}

Problem:  
Solve for $ \nabla_\eta \left( \left< \lambda, \eta^* \right> - G(\eta) \right) = 0 $, specifically show that $ \lambda = E_{ x \sim P( x \vert \eta^*) } \left[ u(x) \right] $ .  Is this a global maximum?  

Using the above formula for $\lambda$, what is $F(\lambda)$?  

What is $\nabla_\lambda F(\lambda)$ ?  This gives us a formula for the canonical link function.

Fenchel's inequality is a direct result of the definition of the convex conjugate

\begin{equation}
  f^*( \lambda) + f(x) \leq \left< \lambda, x \right>
  \label{Fenchel's inequality}
\end{equation}

We observe that the exponential form of an exponential family resembles the term inside the convex conjugate:

\begin{equation}
  P( x \vert \eta) = h(x) e^{ \left< u(x), \eta \right> - G(\eta) } \leq h(x) e^{ F( \bar{u} ) }
  \label{}
\end{equation}

And that it is maximized when $u(x)$ is the expected sufficient statistic.  This means the mean sufficient statistic is related to its mode (disregarding $h(x)$ for now).  

Suppose we were to define an exponential family for the natural parameter:

\begin{equation}
  P( \eta \vert \lambda ) = h^*(\eta) e^{ \left< \lambda, \eta \right> - F(\lambda) }
  \label{}
\end{equation}

We can use Fenchel's inequality for $F(\lambda)$

\begin{equation}
  P( \eta \vert \lambda ) = h^*(\eta) e^{ \left< \lambda, \eta - \eta^* \right> + G(\eta) }
  \label{<++>}
\end{equation}<++>

\section{EM and variational inference}

Free energy?

\end{document}


