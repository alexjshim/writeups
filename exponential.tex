%        File: exponential.tex
%     Created: Tue Feb 16 08:00 PM 2021 P
% Last Change: Tue Feb 16 08:00 PM 2021 P
%
\documentclass[a4paper]{article}
\usepackage[]{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\newtheorem{problem}{Problem}[section]

\begin{document}

\section{Notation}


$X$ random variable \\ 
$x$ instantiation/sample of the random variable $X$ \\
$u(X_1, \ldots, X_N)$ sufficient statistic \\
$\theta$ parameterization of the distribution \\
$\eta$ natural parameter \\
$Z(\eta)$ normalization factor \\
$g(\eta)$ inverse normalizer \\
$h(x)$ intrinsic/carrier measure

\section{Pitman-Koopman-Dermois Theorem}

Suppose a sufficient statistic is additive: $ u(X_1,\ldots,X_N) = \sum_{n=1}^N u(X_n) $.  Furthermore assume it is a minimal sufficient statistic, which means mathematically there exists a mapping from any other sufficient statistic to $u$.

\begin{equation}
  f( u'(X_1,\ldots,X_N) ) = u(X_1,\ldots,X_N) )
  \label{}
\end{equation}

Then we can express parametrize the distribution by the natural parameters, $\eta$, such that

\begin{equation}
  P( x \vert \eta) = g(\eta) h(x) e^{ \left< u(x), \eta \right>}
  \label{}
\end{equation} 

where 

\begin{equation}
  \begin{split}
    \tilde{P}(X) &:= h(x) e^{ \left< u(x), \eta \right> } \\
    Z(\eta) &:= \int h(x) e^{ \left< u(x), \eta \right> } dX \\
    g(\eta) &:= \frac{1}{Z(\eta)}
  \end{split}
  \label{}
\end{equation}

\begin{problem} 
Find a probability distribution that has a sufficient statistic, but is not an exponential family distribution.  Is it an additive sufficient statistic?
\end{problem}

\begin{problem}
  What is the difference between a normal distribution and an exponential family of distributions?
\end{problem}

The following are two more complicated distributions.  Challenge yourself to see if you can convert them to exponential family form.

\begin{problem}
  The Weibull distribution was used to model the distribution for the onset of covid cases.  \\
  The Weibull distribution has the cumulative distribution function
  \begin{equation}
    P( X \leq x \vert \lambda, k ) = \begin{cases}
      1-e^{-\frac{x}{\lambda}^k} &, x \geq 0 \\
      0 &, x <0
    \end{cases}
    \label{}
  \end{equation}

Express the Weibull distribution as an exponential family, with a sufficient statistic and natural parameters.
\end{problem}

\begin{problem}
  The Gumbel distribution, which is used for the Gumbel-softmax trick for backpropagating through categorical distributions, is defined as
  \begin{equation}
    \begin{split}
      P( x \vert \mu, \beta ) = \frac{1}{\beta} e^{-(z+e^{-z})}, \quad z:= \frac{x-\mu}{\beta} \\
      P( X \leq x \vert \mu, \beta ) = e^{-e^{-\frac{x-\mu}{\beta}} }
    \end{split}
    \label{Gumbel}
  \end{equation}
Is it an exponential family?  If so, find a minimal sufficient statistic and natural parameters.
\end{problem}

The following are more conceptual questions.

\begin{problem}
  Bishop shows that the family of Bernoulli distributions is an exponential family (and Bernoulli is a special case of the binomial distribution).  However, there is a mistake/problem with Bishop's definition, what is it??
\end{problem}

\begin{problem}
  Give an exponential family that contains the binomial distribution (discrete distribution of n coin flips with the probability of heads being p).
\end{problem}

\begin{problem}
  Given a pdf, we $p(x \vert \theta)$, we can write $p(x) = e^{\log p( x \vert \theta)}$.  This seems to suggest any pdf can be turned into an exponential family.  But that is not quite correct.  Give conditions when this trick will and will not work.
\end{problem}

\begin{problem}
Show that an exponential family distribution satisfies the Fisher-Neyman factorization theorem

\begin{equation}
  p(x \vert \theta) = h(x) f_\theta( u(x) )
  \label{}
\end{equation}

where $h(x)$,$f_\theta$ are non-negative functions.

\end{problem}


\section{Sufficiency}

This is a more advanced, theoretical section (as in you may want to skip this for later), which is meant to solidify our understanding of the mysterious concept of sufficiency.  An understanding of this is important for ideas about perfect and imperfect data reduction, such as Tishby's Information Bottleneck. \\

One explanation of a sufficient statistic is that it contains all relevant information about observations for inferring the parametrization.  We wish to understand what this means.

\begin{problem}
Suppose we know nothing about our data distribution, $P_{data}(X)$.  What would be an appropriate sufficient statistic?  \\
What do we need to specify beforehand before we can define a sufficient statistic?  
\end{problem}

\begin{problem}
  Suppose we have two sets of samples of size $N$, $\{x_1,\ldots,x_N\},\{x_1',\ldots,x_N'\}$ from a normal distribution $\mathcal{N}( \mu, \sigma^2 )$, where $\sigma^2$ is known but $\mu$ is unknown.  \\
  Further suppose that we have a normal prior, $P(\mu) = \mathcal{N}(0,1)$.  \\
  The sufficient statistic is the sum of the samples, so 
\begin{equation}
  \begin{split}
    u(x_1,\ldots,x_N) = \sum_n x_n \\
    = u(x_1',\ldots,x_N') = \sum_n x_n' \\
  \end{split}
  \label{}
\end{equation}
Is it true that we assume equal posterior probabilities?
\begin{equation}
  P( \mu = 1 \vert x_1,\ldots,x_N ) = P( \mu = 2 \vert x_1', \ldots, x_N')
  \label{}
\end{equation}
Suppose the sufficient statistic is the same.  How do the posteriors compare?
\begin{equation}
  P( \mu = 1 \vert x_1,\ldots,x_N, u(x_1,\ldots,x_N) = C ) = P( \mu = 2 \vert x_1',\ldots,x_N', u(x_1',\ldots,x_N') = C ) ?
  \label{}
\end{equation}

\end{problem}

\begin{problem}
Suppose two sets of samples of size $N$, $\{x_1,\ldots,x_N\},\{x_1',\ldots,x_N'\}$, from a family of distributions parametrized by $\theta$ with sufficient statistic $u$.

Suppose $u(x_1,\ldots,x_N) = u(x_1',\ldots,x_N')$.  Can you find an example where $ \prod_n P(x_n \vert \theta) \neq \prod_n P(x_n \vert \theta') $?
\end{problem}

\begin{problem}
  Suppose now that we now have two gaussian distributions with the same variance, $\mathcal{N}(-1,1)$ and $\mathcal{N}(1,1)$.  We pick with equal probability one of these distributions but without knowing which one, and then we can sample from it repeatedly. \\
Now we repeatedly to sample this distribution $N$ times, but we reject/discard and resample any samples with $x<0$ (but we always end up with a total of $N$ accepted samples).

The sufficient statistic is still the mean (you can try to justify this, though it might be hard).

Can you come up with an example of $\{x_1,\ldots,x_N\}, \{x_1',\ldots,x_N'\}$ with the same sufficient statistic but different likelihoods?
\end{problem}

We can conclude

\begin{equation}
  P( x_1, \ldots, x_N \vert \theta, u(x_1,\ldots,x_N) = C ) \neq P( x_1', \ldots, x_N' \vert \theta, u(x_1',\ldots,x_N') = C )
  \label{}
\end{equation}

So sufficiency does not mean that given all samples with the same sufficient statistic are equally likely.

It does not mean that a sample has the same likelihood regardless of the parametrization (see ancillary statistic).

It does mean that the likelihood ratio of two samples with the same sufficient statistic is the same no matter what the parametrization:

\begin{equation}
  \frac{\prod_n P(x_n \vert \theta) }{ \prod_n P( x_n' \vert \theta) } = \frac{\prod_n P( x_n \vert \theta') }{ \prod_n P( x_n' \vert \theta') }
  \label{}
\end{equation}

That is, $P( x_1, \ldots, x_N \vert u(x_1, \ldots, x_N) \perp \theta $.

It also means that 

\begin{equation}
  \frac{\prod_n P(x_n \vert \theta) }{ \prod_n P( x_n \vert \theta') } = \frac{\prod_n P( x_n' \vert \theta) }{ \prod_n P( x_n' \vert \theta') }
  \label{}
\end{equation}

\begin{problem}
Explain what this equation means in terms of inferring the parameters given the sufficient statistic.
\end{problem}

\begin{problem}
Assuming the above, prove the Fisher-Neyman factorization theorem.

\begin{equation}
  p(x \vert \theta) = h(x) f_\theta( u(x) )
  \label{Fisher-Neyman factorization theorem}
\end{equation}

where $h(x)$,$f_\theta$ are non-negative functions.
\end{problem}

\begin{problem}
Draw a (graphical) relationship between a sample, $\mathcal{D} = X_1,\ldots,X_N)$, the sufficient statistic $u(x_1, \ldots, x_N)$, and the parametrization, $\theta$. 
Use a double forward arrow to indicate deterministic relationships.
\end{problem}


\section{Gradients of exponential families and statistical modeling}

\subsection{Gradients}

The first three problems are very fundamental to exponential families and machine learning, and have some level of progression.

\begin{problem}
Derive an expression for $ \nabla_\eta \log Z(\eta) $ (Hint: in terms of the sufficient statistics). \\
Derive an expression for $ \nabla_\eta \log Z(\eta) $ from the fact that $ \int P( x \vert \eta) dx = 1 $.
\end{problem}

\begin{problem}
Derive an expression for $ \nabla_\eta \nabla_\eta \log Z(\eta) $ in terms of the sufficient statistics.
\end{problem}

\begin{problem}
Derive an expression for $ \nabla_\eta \log P( x \vert \eta ) $ in terms of the expected sufficient statistic. \\
Suppose we have samples $\left\{ X_1, \ldots, X_N \right\}$ from a data distribution $P_{data}(X)$. We can define an empirical distribution, $ P_{emp}(X) = \frac{1}{N} \sum_{n=1}^N \delta(X = X_n) $.  \\
Derive an expression for

\begin{equation}
  \nabla_\eta KL\left( P_{emp}(X) \Vert P( X \vert \eta) \right) = \nabla_\eta \int P_{emp}(X) \log \frac{P_{emp}(X)}{P( X \vert \eta) } dX
  \label{}
\end{equation}
\end{problem}

\subsection{Discriminative Modeling}
The next three problems are increasingly more complex versions of the same problem.  Choose one.

\begin{problem}
  Suppose we have online samples of a Bernoulli distribution with an unknown probability.  Derive a first-order and Newton's method second-order gradient-based update of the distribution parameters.
\end{problem}

\begin{problem}
  Suppose we have a online samples ${(X,Y)}$ for a logistic regression model
  \begin{equation}
    P( Y \vert X, W) = \sigma( \sum_k w_k X_k )
    \label{logistic}
  \end{equation}
  Derive a first order and second order gradient-based update for the parameters $W$.
\end{problem}

\begin{problem} Generalized Linear Models (GLM) \\
  Derive a gradient update for $P_W( Y \vert X) \in Poisson(\lambda) $, where $\eta = \sum_k w_k X_k )$.
\end{problem}

\subsection{Convex Analysis}

The next two problems are more theoretical and are optional if you're not overly concerned about convex analysis.
\begin{problem}
Prove that $ \log Z(\eta)$ is a convex function. 
(Hint: use results from the above problem)  \\
Prove that the domain of natural parameters is convex.  \\
Assuming that the covariance of the sufficient statistic is positive definite ( equivalently, that the variance constrained along any vector is positive), prove that there is a one-to-one relationship between $\eta$ and the expected sufficient statistic.
\end{problem}

\begin{problem}
Assuming the above, prove the canonical link function, $f( E_{ x \sim P( x \vert \eta) } [ u(x) ] ) = \eta $, exists and is one-to-one.
\end{problem}

\subsection{KL divergence, M-projections, and I-projections}

\begin{equation}
  KL \left( P(X) \Vert Q(X) \right) := \int P(X) \log \frac{P(X)}{Q(X)} dX
  \label{KL}
\end{equation}

\begin{problem}
Derive a simplified expression for $ KL\left( P( x \vert \eta') \Vert P( x \vert \eta) \right)$ in terms of the natural parameters and expected sufficient statistics.  \\
What is $ KL\left( \mathcal{N} \left( \mu_1,\Sigma_1 \right) \Vert \mathcal{N} \left( \mu_2, \Sigma_2 \right) \right) $ ?
\end{problem}

\begin{problem}
Moment matching / M-projections \\
What is 
\begin{equation}
  \nabla_\eta KL\left( P( X \vert \eta') \Vert P( X \vert \eta) \right)
  \label{}
\end{equation}
(Hint: do not use the final result of the above problem).
Using gradients / derivatives or the previous problem, derive a condition for the M-projection:

\begin{equation}
  \text{argmin}_\eta KL \left( P_{data}(X) \Vert P( X \vert \eta) \right) = \text{argmin}_\eta \int P_{data}(X) \log \frac{ P_{data}(X) }{ P( X \vert \eta) } dX
  \label{}
\end{equation}
\end{problem}

\begin{problem}
  Derive the M-projection of a probability distribution $P(X)$ on to the space of Gaussian distributions.  \\
  What if you only had samples of a Bernoulli distribution $P(X)$?  How would you estimate the M-projection?
\end{problem}

The first part of this problem is about the ``other'' KL divergence, which is used in variational inference and EM, and when using Boltzmann distributions it ties together ideas in thermodynamics and maximum entropy.

\begin{problem}
  Information projections / I-projections and thermodynamics\\
  I-projections fix the right hand side of a KL divergence and vary the left-hand side. \\
  Suppose are considering the I-projection of an exponential family distribution, over some class of distributions:
  \begin{equation}
    \text{argmin}_{Q(X) \in \mathcal{Q}} KL \left( Q(X) \Vert P( X \vert \eta) \right)
    \label{I-projection}
  \end{equation}
  Assuming $P(X \vert \eta) = g(\eta) e^{\left< u(X), \eta \right> }$, expand the expression for $KL\left( Q(X) \Vert P( X \vert \eta) \right)$ in terms of the entropy of $Q(X)$ and other terms. \\
  For a Boltzmann distribution, the sufficient statistic $u(X) := -\epsilon(X)$ is the negative energy function, $\eta := \frac{1}{T}$ is the inverse temperature, and we use $Z(\eta) = \frac{1}{g(\eta)}$.  Write out $KL \left( Q(X) \Vert P(X \vert \eta) \right)$, and use the fact that it is non-negative to write out an inequality for $- T \log Z(\eta)$. \\
  This is called the Helmholtz free energy.  When the expected energy is constrained to be constant, how does the entropy of $Q(X)$ compare to the Helmholtz free energy? \\
  For what distribution is the Helmholtz free energy maximized (called the Gibbs free energy)?  How does this relate to the maximum entropy distribution?
\end{problem}

\section{Probabilistic Graphical Models}

Undirected Markov networks are defined by $\mathcal{M} = \left( \mathcal{H}, \Phi \right) $ are defined by their undirected graph $\mathcal{H}$ and their factors, $\Phi = \{ \phi_i(D_i) \}_i$.  
The unnormalized probability distribution is called the Gibbs distribution:

\begin{equation}
  \tilde{P}_\Phi(\mathcal{X}) = \prod_i \phi_i(D_i)
  \label{Gibbs distribution}
\end{equation}

\begin{problem}
  If each of the factors is an exponential family distribution (not necessary the same family), write the joint distribution as an exponential family.  \\
\end{problem}

\begin{problem} Log-Linear PGMs
  For positive factors, we could could express themselves in terms of an energy function
  \begin{equation}
    \phi_i(D_i) = e^{ - \epsilon(D)}
    \label{EBM}
  \end{equation}
  For an energy-based model, we assume an energy function $\epsilon(D)$, but we have an additional temperature term, $e^{-\frac{\epsilon(D)}{\tau}}$. \\
  If we take the product of different energy-based models, with different temperatures, what are the sufficient statistics and the natural parameters of the joint distribution?
\end{problem}

\begin{problem}
  Express an Ising model and a restricted Boltzmann machine as a log-linear energy-based model.  What are the energies?  
\end{problem}

Expectation Propagation
\begin{problem}
  Whenever we perform a marginalization, we have to perform an integration, which is not necessarily analytic, or a conjugate prior for the next factor. \\
  Suppose $P(Z) = \mathcal{N}(\mu,\Sigma)$, and $P(X \vert Z)$ is Bernoulli (specifically it is a logistic model).  Suppose we want to approximate the posterior, $P(Z \vert X)$.  We can treat $P(X \vert Z)$ as an unnomralized distribution of $Z$, and make it conjugate to $P(Z)$.  How would we do this, using M-projections?  \\
\end{problem}

\section{Exponential families and convex optimization duality}

This section is focuses on a convex optimization perspective of exponential families and can be skipped.  It is still under construction.

The convex conjugate / Fenchel transform / Legendre-Fenchel transform is the function to a (potentially non-convex) function.

A notational note, we will use lowercase $x$ here for random variables, instead of instantiations, since we are not considering samples in this section.

\begin{equation}
  f^*( \lambda ) = \sup_{x \in X} \left( \left< \lambda, x \right> - f(x) \right)
  \label{convex conjugate}
\end{equation}

Let $G(\eta)$ be the log normalizer:

\begin{equation}
  G(\eta) = \log Z(\eta) = \int h(x) e^{ \left< u(x), \eta \right> } dx
  \label{}
\end{equation}

Then we can re-expresss the exponential family formula as

\begin{equation}
  P( x \vert \eta) = h(x) e^{ \left< u(x), \eta \right> - G(\eta) }
  \label{}
\end{equation}

Let's consider the convex conjugate of the log normalizer:

\begin{equation}
  F( \lambda ) = \sup_\eta \left( \left< \lambda, \eta \right> - G(\eta) \right)
  \label{}
\end{equation}

Problem:  
Solve for $ \nabla_\eta \left( \left< \lambda, \eta^* \right> - G(\eta) \right) = 0 $, specifically show that $ \lambda = E_{ x \sim P( x \vert \eta^*) } \left[ u(x) \right] $ .  Is this a global maximum?  

Using the above formula for $\lambda$, what is $F(\lambda)$?  

What is $\nabla_\lambda F(\lambda)$ ?  This gives us a formula for the canonical link function.

Fenchel's inequality is a direct result of the definition of the convex conjugate

\begin{equation}
  f^*( \lambda) + f(x) \leq \left< \lambda, x \right>
  \label{Fenchel's inequality}
\end{equation}

We observe that the exponential form of an exponential family resembles the term inside the convex conjugate:

\begin{equation}
  P( x \vert \eta) = h(x) e^{ \left< u(x), \eta \right> - G(\eta) } \leq h(x) e^{ F( \bar{u} ) }
  \label{}
\end{equation}

And that it is maximized when $u(x)$ is the expected sufficient statistic.  This means the mean sufficient statistic is related to its mode (disregarding $h(x)$ for now).  

Suppose we were to define an exponential family for the natural parameter:

\begin{equation}
  P( \eta \vert \lambda ) = h^*(\eta) e^{ \left< \lambda, \eta \right> - F(\lambda) }
  \label{}
\end{equation}

We can use Fenchel's inequality for $F(\lambda)$

\begin{equation}
  P( \eta \vert \lambda ) = h^*(\eta) e^{ \left< \lambda, \eta - \eta^* \right> + G(\eta) }
  \label{<++>}
\end{equation}<++>

\section{Conjugate Priors, EM and variational inference}

Bishop 2.4.2

For every exponential family of the form $P( x \vert \eta) = g(\eta) h(x) e^{ \left< u(x), \eta \right>}$, there exists a conjugate prior in the form

\begin{equation}
  P( \eta \vert \chi, \nu) = f( \chi, \nu) g(\eta)^\nu e^{ \nu \left< \eta, \chi \right> }
  \label{conjugate prior}
\end{equation}

\begin{problem}
  What is the normalized posterior distribution for $\eta$, in terms of $f,g,X,\chi,\nu$?  Also, I believe Bishop may have made a mistake later on in chapter 10.4, it should be $\chi_N = \frac{ \sum_n u(x_n) + \nu \chi}{\nu + N }$.
\end{problem}

Challenge problem
\begin{problem}
  Suppose $P(Y \vert X, W)$ is in an exponential family, and assume $\eta = W \phi(X)$.  Come up with a prior for the weights that is conjugate to this exponential family, and derive a formula for the posterior of the weights.
\end{problem}

Expectation Maximization
\begin{problem}
  Suppose $P(X \vert Z)$ is a generalized linear model and $P(Z)$ is a conjugate prior.
\begin{equation}
  \begin{split}
    P( X \vert Z; W) = g( W Z ) h(X) e^{\left< u(X), W Z \right> } \\
    P( Z \vert \chi, \nu) = f(\chi,\nu) g(Z)^\nu e^{\left< Z, \nu \chi \right> }
  \end{split}
  \label{}
\end{equation}
What is the posterior distribution, $P(Z \vert X, W, \chi, \nu)$?  \\
Maximize $W,\chi,\nu$ with respect to $W^{old},\chi^{old},\nu^{old}$.  What expressions do you need to know expectations in order to calculate this maximization?
\end{problem}

Variational Inference
\begin{problem}

\end{problem}

Dynamic Bayesian Networks

\begin{problem}
  Suppose we have a hidden markov model, where $P(X^{(n) \vert Z^{(n)}}$ is from an exponential family, and $P( Z^{(n+1)} \vert Z^{(n)} )$ is based off the conjugate prior (maybe assume it's a linear model, to be similar to Kalman filters).  Derive a forward message passing / belief propagation algorithm.
\end{problem}



\end{document}


