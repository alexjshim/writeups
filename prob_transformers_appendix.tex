%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{enumerate}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2021}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{A Probabilistic Interpretation of Transformers}

\begin{document}

\onecolumn[
\icmltitle{A Probabilistic Interpretation of Transformers}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2021
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{}{}

\begin{icmlauthorlist}
\icmlauthor{Alexander Shim}{ml}
\end{icmlauthorlist}

\icmlaffiliation{ml}{ML Collective}

\icmlcorrespondingauthor{Alexander Shim}{alex.shim@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\bibliography{example_paper}
\bibliographystyle{icml2021}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DELETE THIS PART. DO NOT PLACE CONTENT AFTER THE REFERENCES!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

\section{Proofs}
\label{proofs}

\begin{proposition}
  Let $X = R^D$, $h: X \rightarrow R^{+}$. 
  \begin{enumerate}[(a)]
    \item If $h(x) := \sum_{n=1}^N \delta( x = x_n)$, then 
    $
      \nabla_\eta \log \int h(x) e^{ x^\intercal \eta } dx 
      = \sum_{n=1}^N \frac{ e^{ x_n^\intercal \eta } }{ \sum_{n'} e^{ x_{n'}^\intercal \eta } } x_n 
    $.
    \item If $h(x) = p_0( x \vert \eta_1, \eta_2)$, where $p_0(x \vert \eta_1,\eta_2)$ is the exponential family distribution $p_0(x \vert \eta_1,\eta_2) = \frac{1}{Z_0(\eta}) h_0(x) e^{x^\intercal \eta} e^{u_2(x)^\intercal \eta_2}$, with sufficient statistic $u_1(x) = x$ and arbitrary sufficient statistic $u_2(x)$, natural parameters $(\eta_1,\eta_2)$, intrinsic measure $h_0(x)$, and normalizer $Z_0(\eta_1,\eta_2) = \int h_0(x) e^{ x^\intercal \eta_1 } e^{ u_2(x)^\intercal \eta_2 } dx$, then $\nabla_{_\eta} \log \int h(x) e^{ x^\intercal \eta} = E_{x \sim p_0(x \vert \eta_1 + \eta, \eta_2)} \left[ x \right]$
  \end{enumerate}
  \label{exponential family gradient updates}
\end{proposition}

\begin{proof}
  \begin{enumerate}[(a)]
    \item $Pr \{ x = x_n \} = \frac{ e^{ x_n^\intercal \eta} }{ \sum_{n'} e^{x_{n'}^\intercal \eta } }$, so $\nabla_\eta \log Z(\eta) = E_{x \sim p( x \vert \eta) } \left[ x \right] = \sum_n \frac{ e^{x_n^\intercal \eta } }{ \sum_{n'} e^{x_{n'}^\intercal \eta } } x_n $. \\
    \item We collect exponential terms into a distribution of the same exponential family as $p_0(x \vert \eta_1,\eta_2)$

  \begin{equation}
    \begin{split}
      \int p_0(x) e^{ x^\intercal \eta} dx 
        &= \frac{1}{Z_0(\eta_1,\eta_2)} \int h_0(x) e^{ x^\intercal ( \eta_1 + \eta_2) } e^{u_2(x)^\intercal \eta_2} dx\\
      &= \frac{Z_0(\eta_1+\eta,\eta_2)}{Z_0(\eta_1,\eta_2)} \\
    \end{split}
    \label{}
  \end{equation}

  When evaluating the score, the denominator term has no dependence on $\eta$

  \begin{equation}
    \begin{split}
      \nabla_\eta \log \int p(x \vert \eta_1,\eta_2) e^{ x^\intercal \eta)} dx
	&= \nabla_\eta \log \frac{Z_0(\eta_1+\eta),\eta_2}{Z_0(\eta_1,\eta_2)} \\
      &= \nabla_\eta \log Z_0( \eta_1 + \eta,\eta_2) \\
      &= E_{ x \sim p( x \vert \eta_1 + \eta, \eta_2)} \left[ x \right]
    \end{split}
  \end{equation} 
  \end{enumerate}
\end{proof}

\begin{corollary}
  If $h(x) = \mathcal{N}( x; \mu,\Sigma)$, then $\nabla_\eta \log \int h(x) e^{ x^\intercal \eta} dx = \mu + \Sigma \eta$.
  \label{cor gaussian update}
\end{corollary}

\begin{proof}
  The natural parameter form of a Gaussian is $\frac{1}{Z(\eta_1,\eta_2)} e^{ \left< x, \Sigma^{-1} \mu \right> + \left< x x^\intercal, - \frac{\Sigma^{-1}}{2}\right>}$, where $\left< \, . \, , \, . \, \right>$ denotes the vectorized dot product for both vectors and matrices, with $\eta_1 = \Sigma^{-1} \mu$. Conversely, $\mu = \Sigma \eta_1$. \\
  Hence, $E_{ x \sim p( x \vert \Sigma^{-1} \mu + \eta, - \frac{\Sigma^{-1}}{2}) } \left[ x \right] = \mu + \Sigma \eta$
\end{proof}

\begin{proposition}
  If $h(x) = \sum_{n=1}^N \pi_n \mathcal{N}(\mu_n,\Sigma )$, where $\pi_n \in R^{+}$, then $\nabla_\eta \log \int h(x) e^{ x^\intercal \eta } dx = \Sigma \eta + \sum_{n=1}^N \frac{ \pi_n e^{\mu_n^\intercal \eta} }{ \sum_{n'=1}^N \pi_{n'} e^{ \mu_{n'}^\intercal \eta} } \mu_n$
  \label{rbf update}
\end{proposition}

\begin{proof}
  For a Gaussian $\mathcal{N}(x;\mu,\Sigma)$ for $x \in R^d$, the exponential family normalizer is $(2 \pi)^{\frac{D}{2}} \lvert \Sigma \rvert ^{\frac{1}{2}} e^{\frac{1}{2} \mu^\intercal \Sigma^{-1} \mu }$. \\
  \begin{equation}
    \begin{split}
      \int \sum_{n=1}^N \pi_n \mathcal{N}(x;\mu_n,\Sigma) e^{x^\intercal \eta} dx &= \sum_{n=1}^N \frac{1}{Z(\mu_n,\Sigma)} \pi_n \int e^{ \left< x x^\intercal, - \frac{1}{2} \Sigma^{-1} \right> + \left<x, \Sigma^{-1} \mu_n \right> + x^\intercal \eta} dx \\
      &= \sum_{n=1}^N \pi_n \frac{Z(\mu_n + \Sigma \eta,\Sigma)}{Z(\mu_n,\Sigma)} \\
      &= \sum_{n=1}^N \pi_n e^{\frac{1}{2} (\mu_n + \Sigma \eta)^\intercal \Sigma^{-1} (\mu_n + \Sigma \eta) - \frac{1}{2} \mu_n^\intercal \Sigma^{-1} \mu_n} \\
      &= e^{\frac{1}{2} \eta^\intercal \Sigma \eta} \sum_{n=1}^N \pi_n e^{\mu_n^\intercal \eta}
    \end{split}
    \label{rbf normalizer}
  \end{equation}

  The gradient of the log normalizer simplifies to

  \begin{equation}
    \begin{split}
      \nabla_\eta \log Z(\eta) = \Sigma \eta + \nabla_\eta \log \sum_{n=1}^N \frac{ \pi_n e^{\mu_n^\intercal \eta} }{ \sum_{n'=1}^N \pi_{n'} e^{\mu_{n'}^\intercal \eta }} \mu_n
    \end{split}
    \label{rbf update eq}
  \end{equation}

  which when $\pi_n$ are uniform $\forall n$ is the sum of the discrete attention update and the $\Sigma \eta$ term.
\end{proof}

If our intrinsic measure is a mixture of Gaussians with different covariance matrices, then we can perform a gradient update on a lower bound, $G_{LB}(\eta)$, of the log normalizer.

\begin{proposition}
  If $h(x) = \sum_{n=1}^N \pi_n \mathcal{N} \left( \mu_n, \Sigma_n \right)$, then there exists a lower bound $G_{LB} = \sum_{n=1}^N \pi_n \left( \frac{1}{2} \eta^\intercal \Sigma \eta + \mu_n^\intercal \eta \right) \leq \log Z(\eta)$, with gradient update $\nabla_\eta G_{LB}(\eta) = \sum_{n=1}^N \pi_n ( \mu_n + \Sigma_n \eta )$
  \label{}
\end{proposition}

\begin{proof}
  Using Jensen's inequality on the concave logarithm function,

  \begin{equation}
    \log \int \sum_{n=1}^N \pi_n \mathcal{N} \left( x; \mu_n, \Sigma_n \right) e^{ x^\intercal \eta} dx \geq \sum_{n=1}^N \pi_n \log \int \mathcal{N} ( x; \mu_n, \Sigma_n ) e^{ x^\intercal \eta} dx
    \label{Jensen's inequality}
  \end{equation}

  From \eqref{rbf update eq}, 

  \begin{equation}
    = \sum_{n=1}^N \pi_n \left( \frac{1}{2} \eta^\intercal \Sigma_n \eta + \mu_n^\intercal \eta \right)
    \label{lower bound}
  \end{equation}

  If we perform a gradient update on the lower bound, we have
  
  \begin{equation}
    \nabla_\eta G_{LB}(\eta) = \sum_{n=1}^N \pi_n ( \mu_n + \Sigma_n \eta )
    \label{lower bound update}
  \end{equation}

\end{proof}

Using Corollary $\autoref{cor gaussian update}$, we can apply a generalization of the attention sublayer to update a distribution $p(\eta)$ of natural parameters given a Gaussian intrinsic measure, $h(x) = \mathcal{N}(x;\mu,\Sigma)$. Moreover, if we assume there is a one-to-one transformation of the natural parameters into the intrinsic measure through the activation function, we can prove the stationarity of the attention update operator composed with a renormalization:

\begin{theorem}
  Suppose the distribution of natural parameters is $p_{eq}(\eta) = \mathcal{N}( \Sigma^{-1} \mu, \Sigma^{-1} )$ and we define the intrinsic measure $h(x)$ by a pointwise transformation $ x = \Sigma \eta$. Let $RN_{\eta_0,\Lambda_0}$ be the renormalization operator such that if $E_{ \eta \sim p(\eta) } \left[ ( \eta, \eta \eta^\intercal ) \right] = (\bar{\eta}, \bar{\Sigma}) $, $RN_{\eta_0,\Lambda_0}$ maps $\eta \rightarrow \eta_0 + \Lambda_0^{\frac{1}{2}}\bar{\Sigma}^{-\frac{1}{2}} (\eta - \bar{\eta})$. 
  Then the composition of renormalization operator and the attention update operator $RN_{\Sigma^{-1}\mu,\Sigma^{-1}} \circ A \circ ( p_{eq}(\eta), h(x) ) = p_{eq}(\eta)$ . 
  \label{theorem equilibrium}
\end{theorem}

\begin{proof}
  The attention operator acts pointwise on each $\eta$ by $\eta' = \eta + \nabla_\eta \log \int h(x) e^{x^\intercal \eta} dx$. We can reparametrize $\eta$ as $ \eta = \Sigma^{-1} \mu + \Sigma^{ - \frac{1}{2} } \epsilon$, where $\epsilon \sim \mathcal{N}(0,I)$. Hence $x = \mu + \Sigma^{\frac{1}{2}} \epsilon$, which is the reparametrization of $\mathcal{N}(\mu,\Sigma)$. \\
  By Corollary $\autoref{cor gaussian update}$,
  \begin{equation}
    \eta' = \mu + ( I + \Sigma) \eta
    \label{gaussian attention update}
  \end{equation}
  This is an affine transformation of the natural parameter, and an affine transformation of a gaussian random variable is a gaussian random variable. The renormalization operator is another affine transformation which remaps it to a distribution with mean and covariance of $p_{eq}(\eta)$, and once again it must be a Gaussian distribution.

\end{proof}

\section{Hopfield Networks is All You Need comparison}
\label{hniayn}

One key difference is that for the Hopfield theory, the patterns are fixed, while for our theory the patterns are the changing hidden states. Notably later versions of the Hopfield paper added Hopfield layers with changing patterns, but theoretical proofs of the limiting behavior apply for fixed patterns. Arguably, the Hopfield analysis was not meant to answer how the attention sublayers transformed the input, and the focus on exponential storage capacity focused more on the ability of attention to quickly converge from noisy inputs to stored patterns.

Notably, while the encoder and decoder involve attention sublayers which use the hidden states as patterns, fitting our interpretation better, the decoder layers also add a second attention block which apply attention to the encoder input, fitting the Hopfield paper conditions.

From a conceptual perspective, the advantage of the original Hopfield network was that the patterns were stored in the weights. For modern Hopfield networks, the patterns must be stored separately, and they are not directly related to the transformer weights, removing the benefit of biological plausibility. 

The Hopfield theory requires a more specific set of assumptions, which justify the energy function, whereas our theory is based off of information theoretic ideas of a widely used free energy objective, which is dual to maximum entropy. We argue that our theory is more directly responsible for the theoretic interpretation of the Hopfield energy function and gradient updates and it does not rely on motivating factors behind Hopfield networks.

One mutual deficiency for both theories is that they do not explain the transformer attention weight transformations nor does it explain multihead attention. Not having to consider weight transformations, multihead attention, or FC layers makes equilibrium analysis much simpler. Unfortunately, the only parameters in the attention sublayer are in these weight transformations, requiring all knowledge of the data distribution to be encoded into these parameters - otherwise we might expect our language models to perform fairly well without any weights. However, it may be possible that some weights may be less sensitive or at least easier to train than others - linearly transforming the key and query spacies for the attention probabilities may focus on certain subspaces more than others, potentially leading to more efficient convergence, yet perhaps the Hopfield benefit of exponentially fast convergence may still apply regardless. In contrast, the value weights combine with the multihead attention linear transformation and the fully connected layer, leading to a much more nonconvex problem and training instability.

Dot product approximations for exponential dot product attention have been used, and perhaps those works have some connection to the original Hopfield network Ising model interpretation. Recent work \cite{} has further explored the connection between Hopfield networks and Restricted Boltzmann Machines. Perhaps the FC layer or linear attention can be seen as original Hopfield network layers and exponential dot product attention modern Hopfield network layers.

The Hopfield theory seems to attempt continuous integration of states, which requires a quadratic term in the energy to keep the partition function finite. The quadratic term also seems to fulfill another duty of modeling the skip connection. While we perform a discrete normalization, it is reasonable to assume that our set of hidden states are equivalent to hidden states from a continuous distribution. What this distribution is unclear, though it draws parallels to Arora's random walk of language around a context vector, which may be equivalent to McBal's mean-field approximation interpretation of transformers.



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
