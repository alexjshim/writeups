%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage[]{amsmath,amssymb}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2021}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{A Probabilistic Interpretation of Transformers}

\begin{document}

\twocolumn[
\icmltitle{A Probabilistic Interpretation of Transformers\\
           International Conference on Machine Learning (ICML 2021)}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2021
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Alexander Shim}{}
\end{icmlauthorlist}

\icmlaffiliation{to}{Department of Computation, University of Torontoland, Torontoland, Canada}
\icmlaffiliation{goo}{Googol ShallowMind, New London, Michigan, USA}
\icmlaffiliation{ed}{School of Computation, University of Edenborrow, Edenborrow, United Kingdom}

\icmlcorrespondingauthor{Alexander Shim}{alex.shim@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Transformers, Attention, Contrastive Learning, Exponential Families}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
This document provides a basic paper template and submission guidelines.
Abstracts must be a single paragraph, ideally between 4--6 sentences long.
Gross violations will trigger corrections at the camera-ready phase.
\end{abstract}

\section{Introduction}
\label{introduction}

Transformers have reached state of the art results in language models, significantly outperforming LSTMs. One conceptual explanation for their greater performance is the ability of attention to utilize long range dependencies, whereas Recurrent Neural Networks are limited to encoding past information within a fixed-size hidden state. 
What this explanation does not explain is how certain architectural choices of transformers, specifically exponential dot product attention, also somewhat ambiguously referred to as softmax attention, outperforms alternatives.

Exponential dot product attention has been popularized in contrastive learning and metric learning and is used in state of the art semi-supervised contrastive learning models.
In language models, an exponential dot product probability is used to model conditional probabilities in Word2Vec as well as in some memory networks. 

The successes of transformers has been verified empirically, but little work
has focused upon a theoretical framework for transformers. 
We offer a probabilistic explanation, based off of distributions of the exponential
family, for attention and contrastive probabilities. 
Expressing attention as an exponential family allows us to utilize related theory in statistics, machine learning, and statistical mechanics, offering insightful interpretations of the transformer architecture.

We also explicitly state the limitations of our theory, noting that the modern
Hopfield network interpretation of attention shares many of these limitations. For
some of these limitations, we speculate connections between other areas of re-
search which may reconcile the theoretical inconsistencies, motivating directions
for future research.

\section{Background}
\label{background}

\subsection{Exponential Dot Product Attention}
Word2vec used a skip-gram model to predict neighboring words using a conditional distribution defined by a normalized exponential dot product multinoulli function \cite{DBLP:journals/corr/MikolovSCCD13}

Attention was proposed through normalized exponential alignment functions, often referred to as softmax attention in literature, for Neural Machine Translation \cite{DBLP:journals/corr/Graves13,bahdanau2014neural}, and later work parallelizing computation on sequential data introduced normalized exponential dot product similarity \cite{DBLP:journals/corr/ParikhT0U16} (A Decomposable Attention Model for Natural Language Inference).
Neural Turing Machines gated memory updates using normalized exponential cosine similarity, in what is referred to as soft attention \cite{DBLP:journals/corr/GravesWD14}.

Other transformer precursors parallelized attention updates over the entire sequence into a layer and switched to convolution-based attention weights \cite{kaiser2016neural,DBLP:journals/corr/KaiserB16}. 
The transformer architecture incorporated exponential dot product attention scaled by dimensionality \cite{vaswani2017attention}. 

\subsection{Contrastive Learning and Metric learning}
\label{contrastive}
Noise-Contrastive Estimation (NCE) creates a mixture distribution between real data and noisy data to convert an unsupervised learning problem into a semi-supervised learning problem, modeling the contrastive probabilities as a parametrized logistic distribution \cite{Gutmann2010}.

In metric learning, Multidimensional scaling calculates pairwise distances between projected points \cite{cox2000multidimensional}, which for Euclidean distances are equivalent to calculating covariance matrix terms using dot products \cite{bishop2007}.
Neighborhood Components Analysis learns a low dimensional linear embedding matrix and models a probability of a neighbor by comparing the exponential negative square distance
$
  e^{ - \lVert A x_i - A x_j \rVert^2 }
$
of two input data $x_i,x_j$ to the sum of the exponential negative distances to non-neighbors \cite{conf/nips/GoldbergerRHS04}

Due to slow convergence of Bernoulli contrastive loss and triplet loss, Sohn proposed an exponential dot product probability over multiple examples \cite{Sohn2016ImprovedDM}, which is mathematically consistent with NCE for multiple distributions. The paperâ€™s roots in metric learning motivated the dot product form, with a direct influence from Neighborhood Component Analysis.

More recent contrastive learning research adopted a contrastive loss based off of exponential dot product probabilities, including papers that achieve state of the art semi-supervised learning \cite{wu2018unsupervised,DBLP:journals/corr/abs-2002-05709}.

\subsection{Shortcut Connections and Dynamical Systems}
\label{residual connections}

Long Short-Term Memory (LSTM)  combined a shortcut connection to deal with the vanishing and exploding gradient problem along with gating functions to incorporate and forget information \cite{HochSchm97}. Residual connections similarly formulated the hidden layer as an update to an identity mapping, though without a gating mechanism \cite{he2015deep}. Recurrent Neural Networks have been interpreted as a discrete time approximation to a continuous dynamical system \cite{article}, where gating acts as a warping of time \cite{tallec2018recurrent}. Residual connections have been interpreted as a discretized update to a differential equation \cite{Weinan2017APO,lu2020finite}.

Interpreting residual networks as discretized differential equations, researchers have posed alternative methods for performing forward updates to converge to equilibrium points and backwards updates to the parameters from the equilibrium points \cite{chen2019neural,bai2019deep}. Further work has used monotone operator theory in convex analysis for solving for equilibrium points, interpreting layers as an operator \cite{winston2021monotone}.

In a work most similar to ours, transformers have been interpreted as an update of modern hopfield networks and fixed points have been calculated with respect to a fixed set of patterns \cite{DBLP:journals/corr/abs-2008-02217}.  Our work similarly views the attention sublayer as an operator update over a class of discretized probability distribution, though with a changing set of patterns.

\subsection{Log Normalizer and Free Energy}
\label{log normalizer}

Partition functions, or the normalizer function, in statistical physics defines a normalization factor of the Hamiltonian with respect to a parameter defining the temperature. The Boltzmann distribution can be derived through Lagrange multipliers as the distribution which maximizes entropy with a conservation of energy constraint. Jaynes adapted the Boltzmann distributions to maximum entropy distributions with multiple expected statistics constraints by converting the maximum entropy problem into the dual problem of optimizing the log normalizer \cite{1456693}, which is known in statistical mechanics as free energy.

Variational methods have been used to approximate log probabilities of observations in machine learning, borrowing from ideas in statistical mechanics. By viewing the joint as an unnormalized probability distribution, the log normalizer is known as the evidence lower-bound, and it has connections to Helmholtz Free Energy \cite{Hinton:1995,koller2009probabilistic}.

The sum of exponents loss of AdaBoost \cite{collins00logistic} has been interpreted as the dual form of generalized KL divergence. The log sum of exponents is well known in convex optimization to be the dual form to the maximum entropy objective for a discrete probability distribution \cite{citeulike:163662}. Notably, in the modern Hopfield network interpretation of transformers as part of the energy function \cite{DBLP:journals/corr/abs-2008-02217}.

\section{Exponential Dot Product Attention}
\label{exponential dot product attention}

\subsection{Exponential Families}
\label{exponential families}
The natural parameter form, also known as the canonical form or a linear exponential family, of a distribution of the exponential family can be written as
\begin{equation}
  p(x \vert \eta) = 
    \frac{1}{Z \left( \eta \right) }
    h(x)
    e^{ u(x)^\intercal \eta}
  \label{exponential family}
\end{equation}
where $x$ is the random variable, 
$u(x)$ the sufficient statistic, 
$\eta$ the natural parameter, 
$h(x)$ the intrinsic measure or carrier measure, 
and 
$Z ( \eta ) := \int h(x) e^{ u(x)^\intercal \eta }$ the normalizer or partition function.
An exponential family distribution can be generated from an intrinsic measure $h(x)$ by defining the sufficient statistic $u(x) := x$, and defining the normalizer as the Laplace transform applied to the intrinsic measure:
\begin{equation}
  Z ( \eta ) = \mathcal{L} \circ h(x) = \int h(x) e^{ x^\intercal \eta} dx
  \label{Laplace transform}
\end{equation}
The unnormalized probability mass or probability density may be written as $\tilde{p}(x \vert \eta)$.

When $h(x)$ is chosen to be a uniform discrete measure over a finite set of points 
$\{x_n\}_{n=1}^N $
in a continuous space, the probability density converts to a probability mass function, and the normalizer is the summation 
$Z( \eta ) = \sum_{n=1}^N e^{ u(x_n)^\intercal \eta} $
instead of an integral.

When the query $q_i$ is chosen as the natural parameter and the keys as the finite set of points $\{ k_j \}_{j=1}^N$, exponential dot product attention weights are of the form of an exponential family
\begin{equation}
  p( k_j \vert q_i ) = 
  \frac
    { e^{k_j \cdot q_i } }
    { Z(q_i) }
  \label{exponential family attention}
\end{equation}

The expected sufficient statistic of an exponential family can be written as a one-to-one function of the natural parameter
\begin{equation}
  E_{ x \sim P( x \vert \eta) } \left[ u(x) \right] =
  \nabla_\eta \log Z(\eta)
  \label{activation function}
\end{equation}
For the exponential family defined by attention, we observe that attention averaging is exactly the gradient of the log normalizer
\begin{equation}
  \nabla_\eta \log Z(\eta) \vert_{\eta = q_i} = 
  \sum_{j=1}^N 
    \frac
      { e^{ k_j \cdot q } }
      { \sum_{n'=1}^N e^{k_{n'} \cdot q_i} }
    k_j
  \label{attention gradient update}
\end{equation}

When the attention sublayer is added to the residual connection, we observe that we are performing a gradient update of the log normalizer with respect to the natural parameters, which are the hidden states.

The log normalizer for this discrete distribution is the log sum of exponents, which is a component of the modern Hopfield energy function, where the attention sublayer also acts as an update for the hidden states
\begin{equation}
  \log Z(q_i) = \log \sum_{j=1}^N e^{ k_j \cdot q_i}
  \label{free energy}
\end{equation}

\subsection{Expansion and contraction}
\label{}

The attention sublayer outputs a convex combination of the keys. Without a residual connection, repeated applications of attention would contract the hidden states into the interior, intuitively towards a single fixed point.

With a residual connection, assuming our hidden states are recentered around the origin through layer normalization or some other normalization, we could intuitively imagine that roughly radially symmetric hidden states would push each hidden state further away from the origin.
Since the log normalizer is the dual form of the maximum entropy distribution, our log normalizer ascent should result in increased entropy, resulting in an expansion of the hidden states away from their starting points.

If we are to hope that our transformer layers would converge to a certain distribution, we would require a compensating contractive operator. In transformers, layer normalization performs this contractive operation, and in contrastive learning, cosine similarity projects points back on to the unit sphere.

\section{Operator Interpretation}
\label{operator interpretation}

Starting with a hidden state of $N$ tokens, our attention heads are simultaneous gradient updates on each token. 
We can interpret the tokens as a discretization of a distribution, drawing parallels to the empirical distribution defined by sampling. 
Since we can use a Laplace transform to induce an exponential family from the discrete distribution, we can view the attention sublayer as an operator mapping a distribution of hidden states to a new distribution of hidden states.

Two interesting cases of deriving an exponential family from an intrinsic measure are when a conjugate prior $ p( \eta \vert \xi, \nu )$ is defined as the intrinsic measure, and in boosting where the previous model defines an intrinsic measure of unnormalized weights by $w_i = e^{ - t_i \eta_n(x_i) } $, where $\eta_n$ is the previous iteration of the model logits \cite{collins00logistic}.

The FC layer can be viewed as a discrete approximation to an operator as well, so the entire attention block can be viewed as a distributional operator.  If different attention blocks have tied weights, then it is the same operator applied repeatedly, otherwise the operators for each layer change.

\section{Theoretical Limitations}
\label{limitations}
Both this work and the Hopfield interpretation of transformers ignore the weight matrices, which are the only parameters that the network learns for the attention sublayer. 
With transformer models having hundreds of millions to a billion parameters and being difficult to train, this oversight is deeply problematic. 
The simplest interpretation is for the weight matrices to represent different distortions of space, in which case different attention heads would represent different distortions.
Unfortunately, this makes multihead attention incoherent, as we would be combining gradient updates from different spatial transformations.

For this work, we have interpreted a single update on a single hidden state as a gradient update. 
However, it is less clear why each hidden state has a separate gradient update, and since they depend on the other hidden states, they are not independent updates. The Ising model, which motivated the original Hopfield network, has similar properties. Sample-based approximate inference techniques for Sequential Monte Carlo, particularly ones utilizing bootstrapping, are dependent on the samples and qualitatively similarly use $N$ samples to generate $N$ new samples for the next time step.

The FC layer is inconsistent with both the Hopfield theory of attention and this paper's gradient update of the log normalizer. We would have to view it as a separate operator update composed on top of our attention operator.

\section{Future Work}
\label{future work}
Viewing the attention sublayer as an operator, a natural question is what would the equilbrium distribution of the operator be. 
The Hopfield theory of transformers solved this, but specifically when the patterns were fixed, whereas for transformers the patterns are the dynamically changing hidden states.
A useful experiment would be to sample from an initial distribution and observe how distributions change with each layer, especially with learned weights.


Since the theoretical interpretations ignore the FC layer, it would be useful to know how close the FC layer is to an identity mapping or a purely linear layer, if at all.


Assuming layer normalization should induce an equilibrium distribution of a Gaussian distribution, we could potentially try other contractive mappings to see if this generates substantially different embeddings and layer behavior.

One key difference in theories is that in this work, the residual connection is used as an initial point for a gradient update, whereas the Hopfield theory of attention defines a quadratic term in the Hopfield energy, strongly suggesting that we are working with Gaussian distributions. Since exponential dot product probabilities may come from Euclidean assumptions in metric learning, attention may be making an inherent assumption about marginalization over Gaussian distributions.

In comparison, other exponential families will not map the natural parameter to the expected sufficient statistic as cleanly as with Gaussian distributions. In those cases, we would need to apply a non-linear activation or a link function to transform the sufficient statistic space to the natural parameter space before we take the dot product. An affine approximation of the activation function is the Fisher information matrix, so we could test to see if the query and key weight matrices combine to an approximation of the Fisher information matrix.


\bibliography{prob_transformers}
\bibliographystyle{icml2021}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
